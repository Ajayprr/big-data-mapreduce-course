Last login: Thu Apr 18 14:53:11 on ttys004
mparsian@Mahmouds-MacBook ~ $ cd spark-2.4.0/
mparsian@Mahmouds-MacBook ~/spark-2.4.0 $
mparsian@Mahmouds-MacBook ~/spark-2.4.0 $
mparsian@Mahmouds-MacBook ~/spark-2.4.0 $ ls -l
total 160
-rw-r--r--@   1 mparsian  897801646  21357 Oct 28 23:36 LICENSE
drwxr-xr-x    5 mparsian  897801646    160 Jan  8 02:48 LZO_JARS
-rw-r--r--@   1 mparsian  897801646  42919 Oct 28 23:36 NOTICE
drwxr-xr-x@   3 mparsian  897801646     96 Oct 28 23:36 R
-rw-r--r--@   1 mparsian  897801646   3952 Oct 28 23:36 README.md
-rw-r--r--@   1 mparsian  897801646    156 Oct 28 23:36 RELEASE
-rw-r--r--    1 mparsian  897801646      0 Nov 28 11:39 VERSION-spark-2.4.0-bin-hadoop2.7
drwxr-xr-x@  29 mparsian  897801646    928 Oct 28 23:36 bin
drwxr-xr-x@   9 mparsian  897801646    288 Oct 28 23:36 conf
drwxr-xr-x@   5 mparsian  897801646    160 Oct 28 23:36 data
-rw-r--r--    1 mparsian  897801646    666 Feb 26 18:57 derby.log
drwxr-xr-x@   4 mparsian  897801646    128 Oct 28 23:36 examples
drwxr-xr-x    9 mparsian  897801646    288 Jan  8 02:50 graphframes
drwxr-xr-x@ 227 mparsian  897801646   7264 Oct 28 23:36 jars
drwxr-xr-x@   4 mparsian  897801646    128 Oct 28 23:36 kubernetes
drwxr-xr-x@  48 mparsian  897801646   1536 Oct 28 23:36 licenses
drwxr-xr-x   16 mparsian  897801646    512 Mar 25 12:29 logs
drwxr-xr-x    9 mparsian  897801646    288 Mar 25 12:30 nicloe_hu
drwxr-xr-x    8 mparsian  897801646    256 Apr  3 13:10 nicloe_hu2
drwxr-xr-x@  19 mparsian  897801646    608 Oct 28 23:36 python
drwxr-xr-x@  24 mparsian  897801646    768 Oct 28 23:36 sbin
drwxr-xr-x    2 mparsian  897801646     64 Jan  8 03:00 work
drwxr-xr-x@   3 mparsian  897801646     96 Oct 28 23:36 yarn
drwxr-xr-x   82 mparsian  897801646   2624 Feb 24 12:38 zbin
mparsian@Mahmouds-MacBook ~/spark-2.4.0 $ cat > fox.txt
a fox jumped
a red fox jumped and jumped
a blue and red fox jumped
fox is blue red
mparsian@Mahmouds-MacBook ~/spark-2.4.0 $
mparsian@Mahmouds-MacBook ~/spark-2.4.0 $
mparsian@Mahmouds-MacBook ~/spark-2.4.0 $
mparsian@Mahmouds-MacBook ~/spark-2.4.0 $
mparsian@Mahmouds-MacBook ~/spark-2.4.0 $
mparsian@Mahmouds-MacBook ~/spark-2.4.0 $ cat fox.txt
a fox jumped
a red fox jumped and jumped
a blue and red fox jumped
fox is blue red
mparsian@Mahmouds-MacBook ~/spark-2.4.0 $ ./bin/pyspark
Python 3.7.2 (v3.7.2:9a3ffc0492, Dec 24 2018, 02:44:43)
[Clang 6.0 (clang-600.0.57)] on darwin
Type "help", "copyright", "credits" or "license" for more information.
2019-04-18 18:02:14 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.4.0
      /_/

Using Python version 3.7.2 (v3.7.2:9a3ffc0492, Dec 24 2018 02:44:43)
SparkSession available as 'spark'.
>>> spark
<pyspark.sql.session.SparkSession object at 0x10225b8d0>
>>> records = spark.sparkContex.textFile("/Users/mparsian/spark-2.4.0/fox.txt")
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
AttributeError: 'SparkSession' object has no attribute 'sparkContex'
>>> records = spark.sparkContext.textFile("/Users/mparsian/spark-2.4.0/fox.txt")
>>> records.collect()
['a fox jumped', 'a red fox jumped and jumped', 'a blue and red fox jumped', 'fox is blue red']
>>> records.count()
4
>>>
>>> def tokenize(record):
...     tokens = record.split(" ")
...     return tokens
...
>>>
>>> x = "a fox jumped"
>>> x
'a fox jumped'
>>> tokens = tokenize(x)
>>> tokens
['a', 'fox', 'jumped']
>>>
>>>
>>> words =  records.flatMap(tokenize)
>>> words.collect()
['a', 'fox', 'jumped', 'a', 'red', 'fox', 'jumped', 'and', 'jumped', 'a', 'blue', 'and', 'red', 'fox', 'jumped', 'fox', 'is', 'blue', 'red']
>>> words.count()
19
>>> pairs = words.map(lambda x : (x,1))
>>> pairs.collect()
[('a', 1), ('fox', 1), ('jumped', 1), ('a', 1), ('red', 1), ('fox', 1), ('jumped', 1), ('and', 1), ('jumped', 1), ('a', 1), ('blue', 1), ('and', 1), ('red', 1), ('fox', 1), ('jumped', 1), ('fox', 1), ('is', 1), ('blue', 1), ('red', 1)]
>>> pairs.count()
19
>>>
>>> frequencies = pairs.reduceByKey(lambda a, b: a+b)
>>> frequencies.collect()
/Users/mparsian/spark-2.4.0/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling
/Users/mparsian/spark-2.4.0/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling
[('is', 1), ('a', 3), ('fox', 4), ('jumped', 4), ('red', 3), ('and', 2), ('blue', 2)]
>>>
>>>
>>> filtered = frequencies.filter(lambda x : x[1] > 2)
>>> filtered.collect()
/Users/mparsian/spark-2.4.0/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling
/Users/mparsian/spark-2.4.0/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling
[('a', 3), ('fox', 4), ('jumped', 4), ('red', 3)]
>>> filtered.count()
4
>>> a = ("dada", 5)
>>> a[0]
'dada'
>>> a[1]
5
>>>
>>>
>>> test =  records.map(tokenize)
>>> test.collect()
[['a', 'fox', 'jumped'], ['a', 'red', 'fox', 'jumped', 'and', 'jumped'], ['a', 'blue', 'and', 'red', 'fox', 'jumped'], ['fox', 'is', 'blue', 'red']]
>>> test.count()
4
>>>
>>>
>>> pairs.collect()
[('a', 1), ('fox', 1), ('jumped', 1), ('a', 1), ('red', 1), ('fox', 1), ('jumped', 1), ('and', 1), ('jumped', 1), ('a', 1), ('blue', 1), ('and', 1), ('red', 1), ('fox', 1), ('jumped', 1), ('fox', 1), ('is', 1), ('blue', 1), ('red', 1)]
>>>
>>> grouped = pairs.groupByKey()
>>> grouped.colect()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
AttributeError: 'PipelinedRDD' object has no attribute 'colect'
>>> grouped.collect()
/Users/mparsian/spark-2.4.0/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling
/Users/mparsian/spark-2.4.0/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling
[('is', <pyspark.resultiterable.ResultIterable object at 0x102268da0>), ('a', <pyspark.resultiterable.ResultIterable object at 0x1022c1a90>), ('fox', <pyspark.resultiterable.ResultIterable object at 0x1022c1a58>), ('jumped', <pyspark.resultiterable.ResultIterable object at 0x1022c1be0>), ('red', <pyspark.resultiterable.ResultIterable object at 0x1022c1b00>), ('and', <pyspark.resultiterable.ResultIterable object at 0x1022c1dd8>), ('blue', <pyspark.resultiterable.ResultIterable object at 0x1022c1d30>)]
>>>
>>> grouped = pairs.groupByKey().mapValues(lambda it: list(it))
>>> grouped.collect()
/Users/mparsian/spark-2.4.0/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling
/Users/mparsian/spark-2.4.0/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling
[('is', [1]), ('a', [1, 1, 1]), ('fox', [1, 1, 1, 1]), ('jumped', [1, 1, 1, 1]), ('red', [1, 1, 1]), ('and', [1, 1]), ('blue', [1, 1])]
>>> grouped = pairs.groupByKey()
>>> grouped.collect()
/Users/mparsian/spark-2.4.0/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling
/Users/mparsian/spark-2.4.0/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling
[('is', <pyspark.resultiterable.ResultIterable object at 0x1022d2400>), ('a', <pyspark.resultiterable.ResultIterable object at 0x1022d2470>), ('fox', <pyspark.resultiterable.ResultIterable object at 0x1022d2438>), ('jumped', <pyspark.resultiterable.ResultIterable object at 0x1022d25c0>), ('red', <pyspark.resultiterable.ResultIterable object at 0x1022d24e0>), ('and', <pyspark.resultiterable.ResultIterable object at 0x1022d26d8>), ('blue', <pyspark.resultiterable.ResultIterable object at 0x1022d2748>)]
>>> freq2 = grouped.mapValues(lambda it: sum(it))
>>> freq2.collect()
/Users/mparsian/spark-2.4.0/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling
/Users/mparsian/spark-2.4.0/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling
[('is', 1), ('a', 3), ('fox', 4), ('jumped', 4), ('red', 3), ('and', 2), ('blue', 2)]
>>> freq2.count()
7
>>> frequencies = records.flatMap(tokenize).map(lambda x: (x,1)).reduceByKey(lambda a, b: a+b)
>>> frequencies.collect()
/Users/mparsian/spark-2.4.0/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling
/Users/mparsian/spark-2.4.0/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling
[('is', 1), ('a', 3), ('fox', 4), ('jumped', 4), ('red', 3), ('and', 2), ('blue', 2)]
>>>
